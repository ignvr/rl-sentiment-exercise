{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Fine-tuning for Positive Sentiment Generation\n",
    "\n",
    "## Exercise: Implementing Reward Functions for GRPO\n",
    "\n",
    "In this exercise, you will learn how to fine-tune a language model using **Reinforcement Learning** to generate text with positive sentiment. Specifically, you'll implement different reward functions and observe how they affect the trained model's behavior.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will:\n",
    "1. Understand how reward functions guide RL training\n",
    "2. Implement a basic sentiment-based reward function\n",
    "3. Explore reward shaping techniques (exponential)\n",
    "4. Understand KL divergence regularization (forward vs backward)\n",
    "5. Compare different training configurations empirically\n",
    "\n",
    "### Background\n",
    "\n",
    "We use **GRPO (Group Relative Policy Optimization)** from the TRL library. GRPO works by:\n",
    "1. Generating multiple completions for each prompt\n",
    "2. Computing rewards for each completion\n",
    "3. Computing advantages relative to the group\n",
    "4. Updating the policy to increase probability of high-advantage completions\n",
    "\n",
    "The key insight is that the **reward function determines what the model learns**. A simple sentiment classifier reward will push the model toward positive text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install dependencies and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab or without conda setup, uncomment to install:\n",
    "# !pip install transformers trl torch datasets accelerate matplotlib rich\n",
    "#\n",
    "# For local setup with conda (recommended), see README.md:\n",
    "# conda create -n sentiment python=3.10 -y && conda activate sentiment\n",
    "# pip install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our modules\n",
    "from data import get_train_dataset, get_validation_dataset, VALIDATION_PROMPTS\n",
    "from sentiment import load_sentiment_model, get_sentiment_scores\n",
    "\n",
    "# Load the sentiment model (we'll use this throughout)\n",
    "sentiment_model, sentiment_tokenizer = load_sentiment_model()\n",
    "print(\"Sentiment model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Sentiment Classifier\n",
    "\n",
    "Before implementing reward functions, let's understand how the sentiment classifier works. We use `nlptown/bert-base-multilingual-uncased-sentiment`, a 5-star rating model. We compute the expected star rating and rescale to [0, 1]: score = (E[stars] - 1) / 4. This gives more continuous scores compared to binary classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sentiment classifier\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! I loved every moment.\",\n",
    "    \"Terrible film. Complete waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"The acting was superb and the story was compelling.\",\n",
    "    \"Boring and predictable. I almost fell asleep.\"\n",
    "]\n",
    "\n",
    "scores = get_sentiment_scores(test_texts)\n",
    "\n",
    "print(\"Sentiment Scores (P(positive)):\")\n",
    "print(\"-\" * 60)\n",
    "for text, score in zip(test_texts, scores):\n",
    "    sentiment = \"POSITIVE\" if score > 0.5 else \"NEGATIVE\"\n",
    "    print(f\"{score:.3f} [{sentiment:8s}]: {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Basic Sentiment Reward\n",
    "\n",
    "Your first task is to implement a basic sentiment reward function. The reward should simply be the probability of positive sentiment for each completion.\n",
    "\n",
    "**Task**: Implement `sentiment_reward()` in the cell below.\n",
    "\n",
    "**Hint**: Use the `get_sentiment_scores()` helper function which returns P(positive) for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_reward(completions: list[str], **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Basic sentiment reward function.\n",
    "    \n",
    "    Computes the reward as the probability of positive sentiment for each completion.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of generated text completions\n",
    "        **kwargs: Additional arguments (not used here)\n",
    "    \n",
    "    Returns:\n",
    "        List of reward values in [0, 1], one per completion\n",
    "    \n",
    "    Example:\n",
    "        >>> rewards = sentiment_reward([\"Great movie!\", \"Terrible film.\"])\n",
    "        >>> # rewards should be approximately [0.99, 0.02]\n",
    "    \"\"\"\n",
    "    # =========================================================================\n",
    "    # YOUR CODE HERE (1-2 lines)\n",
    "    # Hint: Use get_sentiment_scores(completions)\n",
    "    # =========================================================================\n",
    "    \n",
    "    raise NotImplementedError(\"Implement sentiment_reward\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # END YOUR CODE\n",
    "    # ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "test_completions = [\n",
    "    \"This movie was amazing and I loved it!\",\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"This movie was okay I guess.\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    rewards = sentiment_reward(test_completions)\n",
    "    print(\"Your sentiment_reward implementation:\")\n",
    "    for text, reward in zip(test_completions, rewards):\n",
    "        print(f\"  {reward:.3f}: {text}\")\n",
    "    \n",
    "    # Basic validation\n",
    "    assert len(rewards) == 3, \"Should return one reward per completion\"\n",
    "    assert all(0 <= r <= 1 for r in rewards), \"Rewards should be in [0, 1]\"\n",
    "    assert rewards[0] > rewards[1], \"Positive text should have higher reward\"\n",
    "    print(\"\\n✓ Tests passed!\")\n",
    "except NotImplementedError:\n",
    "    print(\"❌ Not implemented yet - complete the function above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: Reward Shaping\n",
    "\n",
    "Raw sentiment probabilities might not be the optimal reward signal. **Reward shaping** transforms the raw reward to potentially improve learning.\n",
    "\n",
    "**Note on GRPO**: For algorithms like GRPO that use *relative* comparisons within groups, linear transformations (shift and scale) don't change the learning signal - they're mathematically equivalent to no shaping. Only *non-linear* transformations like exponential shaping can change the relative differences between rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear shaping (scale * (score - baseline)) is mathematically equivalent\n",
    "# to no shaping for GRPO, since it uses relative comparisons.\n",
    "# We only implement exponential shaping which changes relative differences.\n",
    "print(\"Skipping linear shaping - see note above about GRPO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed to exponential shaping below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Reward Shaping\n",
    "\n",
    "Exponential shaping creates a **non-linear** reward curve:\n",
    "\n",
    "$$\\text{reward} = \\exp(\\text{score} / \\text{temperature}) - 1$$\n",
    "\n",
    "Unlike linear shaping, this changes the *relative* differences between rewards:\n",
    "- Amplifies differences at the high end (very positive completions get much higher rewards)\n",
    "- Compresses differences at the low end\n",
    "\n",
    "The **temperature** parameter controls steepness:\n",
    "- Lower temperature → sharper exponential curve (more differentiation)\n",
    "- Higher temperature → flatter curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaped_reward_exponential(\n",
    "    completions: list[str],\n",
    "    temperature: float = 1.0,\n",
    "    **kwargs\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Exponential reward shaping: reward = exp(score / temperature) - 1\n",
    "    \n",
    "    Args:\n",
    "        completions: List of generated text completions\n",
    "        temperature: Controls steepness (lower = steeper)\n",
    "    \n",
    "    Returns:\n",
    "        List of shaped reward values\n",
    "    \"\"\"\n",
    "    # =========================================================================\n",
    "    # YOUR CODE HERE (2-3 lines)\n",
    "    # 1. Get sentiment scores\n",
    "    # 2. Apply: exp(score / temperature) - 1\n",
    "    # Hint: Use math.exp() for the exponential\n",
    "    # =========================================================================\n",
    "    \n",
    "    raise NotImplementedError(\"Implement shaped_reward_exponential\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # END YOUR CODE\n",
    "    # ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test exponential shaping\n",
    "try:\n",
    "    rewards = shaped_reward_exponential(test_completions)\n",
    "    print(\"Exponential shaped rewards (temperature=1.0):\")\n",
    "    for text, reward in zip(test_completions, rewards):\n",
    "        print(f\"  {reward:.3f}: {text}\")\n",
    "    \n",
    "    # All rewards should be non-negative (exp(x) >= 1 for x >= 0, so exp(x) - 1 >= 0)\n",
    "    assert all(r >= 0 for r in rewards), \"Exponential rewards should be >= 0\"\n",
    "    print(\"\\n✓ Tests passed!\")\n",
    "except NotImplementedError:\n",
    "    print(\"❌ Not implemented yet - complete the function above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Reward Shaping\n",
    "\n",
    "Let's visualize how different reward shaping methods transform the sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reward shaping curves\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "# Compute different transformations\n",
    "y_raw = x  # No shaping\n",
    "y_exp_1 = np.exp(x / 1.0) - 1  # Exponential, temp=1.0\n",
    "y_exp_05 = np.exp(x / 0.5) - 1  # Exponential, temp=0.5 (sharper)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y_raw, label='Raw (no shaping)', linewidth=2)\n",
    "plt.plot(x, y_exp_1, label='Exponential (temp=1.0)', linewidth=2)\n",
    "plt.plot(x, y_exp_05, label='Exponential (temp=0.5)', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Neutral')\n",
    "\n",
    "plt.xlabel('Sentiment Score (P(positive))')\n",
    "plt.ylabel('Shaped Reward')\n",
    "plt.title('Reward Shaping Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: KL Divergence Penalties (Advanced)\n",
    "\n",
    "**KL divergence** measures how different two probability distributions are. In RL fine-tuning, we often add a KL penalty to prevent the policy from deviating too far from the original (reference) model.\n",
    "\n",
    "There are two ways to compute KL divergence:\n",
    "\n",
    "## Forward KL: $D_{KL}(\\pi_{\\text{policy}} || \\pi_{\\text{ref}})$\n",
    "\n",
    "- Penalizes the policy for putting mass where the reference doesn't\n",
    "- \"Mode-covering\" behavior: tries to cover all modes of reference\n",
    "- Tends to produce more diverse outputs\n",
    "\n",
    "## Backward KL: $D_{KL}(\\pi_{\\text{ref}} || \\pi_{\\text{policy}})$\n",
    "\n",
    "- Penalizes the policy for NOT having mass where the reference does\n",
    "- \"Mode-seeking\" behavior: focuses on main modes of reference\n",
    "- Can lead to mode collapse but more focused outputs\n",
    "\n",
    "**Your Task**: Implement `compute_ref_log_probs`, `kl_penalty_forward`, and `kl_penalty_backward` in `rewards.py`. Then train with `--kl_type forward` or `--kl_type backward` to use your implementations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the KL exercises, we need to understand log probabilities\n",
    "# Let's first see how to compute them\n",
    "\n",
    "# Load a small GPT-2 model\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Example: compute log probability of a sequence\n",
    "text = \"This movie was great!\"\n",
    "inputs = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2_model(**inputs)\n",
    "    logits = outputs.logits  # Shape: (batch, seq_len, vocab_size)\n",
    "    \n",
    "    # Convert to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get log prob of each token (shifted by 1 because logits[t] predicts token[t+1])\n",
    "    token_ids = inputs[\"input_ids\"][0]\n",
    "    seq_log_prob = 0.0\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {gpt2_tokenizer.convert_ids_to_tokens(token_ids)}\")\n",
    "    print(\"\\nPer-token log probabilities:\")\n",
    "    for t in range(len(token_ids) - 1):\n",
    "        next_token = token_ids[t + 1]\n",
    "        token_log_prob = log_probs[0, t, next_token].item()\n",
    "        seq_log_prob += token_log_prob\n",
    "        print(f\"  P({gpt2_tokenizer.decode(next_token):10s}) = {math.exp(token_log_prob):.4f} (log: {token_log_prob:.2f})\")\n",
    "    \n",
    "    print(f\"\\nTotal sequence log prob: {seq_log_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding KL Penalty in Practice\n",
    "\n",
    "KL regularization prevents the model from drifting too far from the original GPT-2.\n",
    "\n",
    "You will implement custom KL in the reward function (via `--kl_type` parameter):\n",
    "- Adds a bonus/penalty to the reward based on reference model probability\n",
    "- **Forward KL**: `reward += kl_coef * log(P_ref)` (bonus for likely outputs)\n",
    "- **Backward KL**: `reward -= kl_coef * exp(-log(P_ref))` (penalty for unlikely outputs)\n",
    "\n",
    "Implement `compute_ref_log_probs`, `kl_penalty_forward`, and `kl_penalty_backward` in `rewards.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: kl_penalty_forward signature (implement in rewards.py)\n",
    "#\n",
    "# def kl_penalty_forward(\n",
    "#     completions: list[str],\n",
    "#     prompts: list[str],\n",
    "#     ref_model,\n",
    "#     tokenizer,\n",
    "#     kl_coef: float = 0.1,\n",
    "#     **kwargs\n",
    "# ) -> list[float]:\n",
    "#     \"\"\"\n",
    "#     Forward KL: reward += kl_coef * log(P_ref)\n",
    "#     Bonus for outputs likely under reference model.\n",
    "#     \"\"\"\n",
    "#     log_probs = compute_ref_log_probs(completions, prompts, ref_model, tokenizer)\n",
    "#     return [kl_coef * lp for lp in log_probs]\n",
    "\n",
    "print('See rewards.py for the full implementation exercise.')\n",
    "print('Key functions to implement:')\n",
    "print('  - compute_ref_log_probs(): Compute log P(completion | prompt) under ref model')\n",
    "print('  - kl_penalty_forward(): Return kl_coef * log_prob (bonus for likely outputs)')\n",
    "print('  - kl_penalty_backward(): Return -kl_coef * exp(-log_prob) (penalty for unlikely outputs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Running Training Experiments\n",
    "\n",
    "Now let's use your implemented reward functions to train models with different configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what the BASE model generates (before training)\n",
    "from evaluate import generate_completions, load_model\n",
    "\n",
    "base_model, base_tokenizer = load_model(\"gpt2\")\n",
    "\n",
    "sample_prompts = [\n",
    "    \"This movie was\",\n",
    "    \"The acting in this film\",\n",
    "    \"I watched this yesterday and\",\n",
    "    \"The story was\",\n",
    "    \"Overall, I think this movie\"\n",
    "]\n",
    "\n",
    "print(\"BASE GPT-2 Generations (before training):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_completions = generate_completions(base_model, base_tokenizer, sample_prompts)\n",
    "base_scores = get_sentiment_scores(base_completions)\n",
    "\n",
    "for prompt, completion, score in zip(sample_prompts, base_completions, base_scores):\n",
    "    sentiment = \"POS\" if score > 0.5 else \"NEG\"\n",
    "    print(f\"\\n[{score:.2f} {sentiment}] {completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot base model sentiment distribution\n",
    "from data import TRAIN_PROMPTS\n",
    "\n",
    "# Generate more samples for statistics\n",
    "all_base_completions = generate_completions(base_model, base_tokenizer, TRAIN_PROMPTS[:20])\n",
    "all_base_scores = get_sentiment_scores(all_base_completions)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(all_base_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Neutral')\n",
    "plt.axvline(x=np.mean(all_base_scores), color='g', linestyle='-', \n",
    "            label=f'Mean: {np.mean(all_base_scores):.2f}')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Base GPT-2 Sentiment Distribution (Before Training)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Base model statistics:\")\n",
    "print(f\"  Mean sentiment: {np.mean(all_base_scores):.3f}\")\n",
    "print(f\"  Std: {np.std(all_base_scores):.3f}\")\n",
    "print(f\"  Positive ratio: {np.mean(np.array(all_base_scores) > 0.5):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Different Configurations\n",
    "\n",
    "Now let's train with your reward functions! You can experiment with:\n",
    "\n",
    "1. **Basic sentiment reward** (your `sentiment_reward`)\n",
    "2. **Exponential shaping** (your `shaped_reward_exponential`)\n",
    "3. **Exponential shaping** (your `shaped_reward_exponential`)\n",
    "4. **Custom KL regularization** (your `kl_penalty_forward` / `kl_penalty_backward`)\n",
    "\n",
    "Run the cells below to train different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with basic sentiment reward\n",
    "# This will take ~5-10 minutes depending on your hardware\n",
    "\n",
    "from train import train\n",
    "\n",
    "# Uncomment to run training:\n",
    "# trainer = train(\n",
    "#     model_name=\"gpt2\",\n",
    "#     output_dir=\"./outputs/basic_sentiment\",\n",
    "#     preset=\"quick\",  # Use \"medium\" for better results\n",
    "#     reward_shaping=\"none\",\n",
    "# )\n",
    "\n",
    "print(\"Training code ready. Uncomment and run to start training!\")\n",
    "print(\"Estimated time: ~3-5 minutes for 'quick' preset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with exponential shaping\n",
    "\n",
    "# Uncomment to run training:\n",
    "# trainer = train(\n",
    "#     model_name=\"gpt2\",\n",
    "#     output_dir=\"./outputs/exponential_shaping\",\n",
    "#     preset=\"quick\",\n",
    "#     reward_shaping=\"exponential\",\n",
    "# )\n",
    "\n",
    "print(\"Training code ready. Uncomment and run to start training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with custom KL regularization (uses your implementation!)\n",
    "\n",
    "# Uncomment to run training with Forward KL:\n",
    "# trainer = train(\n",
    "#     model_name=\"gpt2\",\n",
    "#     output_dir=\"./outputs/with_forward_kl\",\n",
    "#     preset=\"quick\",\n",
    "#     reward_shaping=\"none\",\n",
    "#     kl_type=\"forward\",  # Uses your kl_penalty_forward!\n",
    "#     kl_coef=0.1,        # Regularization strength\n",
    "# )\n",
    "\n",
    "# Or try Backward KL:\n",
    "# trainer = train(\n",
    "#     model_name=\"gpt2\",\n",
    "#     output_dir=\"./outputs/with_backward_kl\",\n",
    "#     preset=\"quick\",\n",
    "#     reward_shaping=\"none\",\n",
    "#     kl_type=\"backward\",  # Uses your kl_penalty_backward!\n",
    "#     kl_coef=0.1,\n",
    "# )\n",
    "\n",
    "print(\"Training code ready. Uncomment and run to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Trained Models\n",
    "\n",
    "After training, compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base vs trained model (after training completes)\n",
    "# Uncomment and modify paths as needed:\n",
    "\n",
    "# from evaluate import compare_models, plot_comparison, print_comparison_samples\n",
    "\n",
    "# comparison = compare_models(\n",
    "#     base_model=\"gpt2\",\n",
    "#     trained_model=\"./outputs/basic_sentiment/final\",\n",
    "#     num_samples=10\n",
    "# )\n",
    "\n",
    "# print_comparison_samples(comparison)\n",
    "# plot_comparison(comparison)\n",
    "\n",
    "print(\"Evaluation code ready. Run after training completes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Analysis Questions\n",
    "\n",
    "After completing the exercises, consider these questions:\n",
    "\n",
    "1. **Reward Function Impact**: How did different reward shaping methods affect:\n",
    "   - Training speed (reward curve slope)\n",
    "   - Final sentiment scores\n",
    "   - Generation diversity\n",
    "\n",
    "2. **KL Divergence Trade-offs**: When using KL regularization (`--kl_type forward` or `backward`):\n",
    "   - Does the model stay closer to the base model's behavior?\n",
    "   - Is there a trade-off between sentiment positivity and text quality?\n",
    "   - What happens with very high `kl_coef` values?\n",
    "\n",
    "3. **Mode Collapse**: Did you observe any signs of mode collapse (repetitive outputs)? How did different configurations affect this?\n",
    "\n",
    "4. **Forward vs Backward KL**: (If implemented) How do forward and backward KL penalties differ in their effects on generation?\n",
    "\n",
    "Write your observations in the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Observations\n",
    "\n",
    "*Double-click to edit this cell and write your analysis...*\n",
    "\n",
    "**Reward Shaping Observations:**\n",
    "- \n",
    "\n",
    "**KL Penalty Observations:**\n",
    "- \n",
    "\n",
    "**Mode Collapse Observations:**\n",
    "- \n",
    "\n",
    "**Other Findings:**\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus Challenges\n",
    "\n",
    "If you finish early, try these extensions:\n",
    "\n",
    "1. **Custom Reward Shaping**: Design your own non-linear reward transformation. Can you find one that works better than exponential?\n",
    "\n",
    "2. **Reward Combination**: What happens if you combine sentiment reward with a length penalty? Implement and test.\n",
    "\n",
    "3. **Temperature Experiments**: Try different sampling temperatures during generation. How does this affect the sentiment/quality trade-off?\n",
    "\n",
    "4. **Different Base Models**: Try using `gpt2-medium` instead of `gpt2`. How does model size affect training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Custom reward function\n",
    "\n",
    "def my_custom_reward(completions: list[str], **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Your custom reward function.\n",
    "    \n",
    "    Ideas to try:\n",
    "    - Combine sentiment with length penalty\n",
    "    - Add bonus for certain keywords\n",
    "    - Penalize repetition\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **GRPO Algorithm**: How Group Relative Policy Optimization trains LLMs using RL\n",
    "2. **Reward Functions**: The crucial role of reward design in RL fine-tuning\n",
    "3. **Reward Shaping**: How exponential transformations affect learning (linear is equivalent to none for GRPO)\n",
    "4. **KL Regularization**: Using KL divergence to prevent catastrophic forgetting\n",
    "5. **Practical Training**: Running and evaluating RL fine-tuning experiments\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The reward function is the most important design choice in RL fine-tuning\n",
    "- Reward shaping can significantly impact training dynamics\n",
    "- KL penalties help maintain model quality while optimizing for the task\n",
    "- There's always a trade-off between task performance and generation diversity\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "- [DeepSeekMath Paper (GRPO)](https://arxiv.org/abs/2402.03300)\n",
    "- [KL Approximation (Schulman)](http://joschu.net/blog/kl-approx.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
